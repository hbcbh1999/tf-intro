{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 22: TensorFlow (workshop)\n",
    "### Presented by [Jeff Heaton, RGA](https://sites.wustl.edu/jeffheaton/)\n",
    "For more examples like these, refer to my [class website](https://sites.wustl.edu/jeffheaton/t81-558/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:14:59) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "\n",
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "# Do you have TensorFlow installed?\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow as a Computation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Very simple calculation.\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "matrix1 = tf.constant([[3., 3.]]) \n",
    "matrix2 = tf.constant([[2.],[2.]]) \n",
    "product = tf.matmul(matrix1, matrix2) \n",
    "\n",
    "with tf.Session() as sess: \n",
    "    result = sess.run([product]) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n",
      "[ 1.  3.]\n"
     ]
    }
   ],
   "source": [
    "# With variables.\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "x.initializer.run()\n",
    "\n",
    "sub = tf.subtract(x, a)\n",
    "print(sub.eval())\n",
    "\n",
    "sess.run(x.assign([4.0, 6.0])) \n",
    "print(sub.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandelbrot in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nKKKACiiigAooooAKKKKACiiigAooooA//9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Imports for visualization\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def DisplayFractal(a, fmt='jpeg'):\n",
    "  \"\"\"Display an array of iteration counts as a\n",
    "     colorful picture of a fractal.\"\"\"\n",
    "  a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1])\n",
    "  img = np.concatenate([10+20*np.cos(a_cyclic),\n",
    "                        30+50*np.sin(a_cyclic),\n",
    "                        155-80*np.cos(a_cyclic)], 2)\n",
    "  img[a==a.max()] = 0\n",
    "  a = img\n",
    "  a = np.uint8(np.clip(a, 0, 255))\n",
    "  f = BytesIO()\n",
    "  PIL.Image.fromarray(a).save(f, fmt)\n",
    "  display(Image(data=f.getvalue()))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Use NumPy to create a 2D array of complex numbers\n",
    "\n",
    "Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]\n",
    "Z = X+1j*Y\n",
    "\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "zs = tf.Variable(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Compute the new values of z: z^2 + x\n",
    "zs_ = zs*zs + xs\n",
    "\n",
    "# Have we diverged with this new value?\n",
    "not_diverged = tf.abs(zs_) < 4\n",
    "\n",
    "step = tf.group(\n",
    "  zs.assign(zs_),\n",
    "  ns.assign_add(tf.cast(not_diverged, tf.float32))\n",
    "  )\n",
    "\n",
    "for i in range(200): step.run()\n",
    "    \n",
    "DisplayFractal(ns.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions for Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/1000\n",
      "0s - loss: 1.0984 - val_loss: 1.0987\n",
      "Epoch 2/1000\n",
      "0s - loss: 1.0927 - val_loss: 1.0937\n",
      "Epoch 3/1000\n",
      "0s - loss: 1.0881 - val_loss: 1.0907\n",
      "Epoch 4/1000\n",
      "0s - loss: 1.0851 - val_loss: 1.0880\n",
      "Epoch 5/1000\n",
      "0s - loss: 1.0820 - val_loss: 1.0852\n",
      "Epoch 6/1000\n",
      "0s - loss: 1.0789 - val_loss: 1.0824\n",
      "Epoch 7/1000\n",
      "0s - loss: 1.0751 - val_loss: 1.0789\n",
      "Epoch 8/1000\n",
      "0s - loss: 1.0711 - val_loss: 1.0748\n",
      "Epoch 9/1000\n",
      "0s - loss: 1.0666 - val_loss: 1.0701\n",
      "Epoch 10/1000\n",
      "0s - loss: 1.0619 - val_loss: 1.0655\n",
      "Epoch 11/1000\n",
      "0s - loss: 1.0563 - val_loss: 1.0598\n",
      "Epoch 12/1000\n",
      "0s - loss: 1.0504 - val_loss: 1.0533\n",
      "Epoch 13/1000\n",
      "0s - loss: 1.0440 - val_loss: 1.0464\n",
      "Epoch 14/1000\n",
      "0s - loss: 1.0369 - val_loss: 1.0386\n",
      "Epoch 15/1000\n",
      "0s - loss: 1.0294 - val_loss: 1.0297\n",
      "Epoch 16/1000\n",
      "0s - loss: 1.0209 - val_loss: 1.0206\n",
      "Epoch 17/1000\n",
      "0s - loss: 1.0121 - val_loss: 1.0103\n",
      "Epoch 18/1000\n",
      "0s - loss: 1.0024 - val_loss: 0.9990\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.9918 - val_loss: 0.9872\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.9806 - val_loss: 0.9750\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.9692 - val_loss: 0.9617\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.9575 - val_loss: 0.9485\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.9451 - val_loss: 0.9354\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.9321 - val_loss: 0.9218\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.9185 - val_loss: 0.9072\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.9043 - val_loss: 0.8921\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.8900 - val_loss: 0.8760\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.8757 - val_loss: 0.8594\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.8602 - val_loss: 0.8435\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.8450 - val_loss: 0.8270\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.8296 - val_loss: 0.8092\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.8138 - val_loss: 0.7915\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.7986 - val_loss: 0.7741\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.7825 - val_loss: 0.7582\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.7672 - val_loss: 0.7406\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.7522 - val_loss: 0.7233\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.7370 - val_loss: 0.7068\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.7222 - val_loss: 0.6913\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.7077 - val_loss: 0.6759\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.6944 - val_loss: 0.6598\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.6805 - val_loss: 0.6453\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.6679 - val_loss: 0.6315\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.6556 - val_loss: 0.6176\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.6435 - val_loss: 0.6050\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.6320 - val_loss: 0.5932\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.6215 - val_loss: 0.5818\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.6110 - val_loss: 0.5714\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.6018 - val_loss: 0.5605\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.5921 - val_loss: 0.5508\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.5835 - val_loss: 0.5415\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.5751 - val_loss: 0.5331\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.5675 - val_loss: 0.5247\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.5598 - val_loss: 0.5168\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.5531 - val_loss: 0.5093\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.5463 - val_loss: 0.5025\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.5399 - val_loss: 0.4961\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.5341 - val_loss: 0.4901\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.5286 - val_loss: 0.4840\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.5233 - val_loss: 0.4784\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.5180 - val_loss: 0.4734\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.5135 - val_loss: 0.4683\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.5088 - val_loss: 0.4636\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.5046 - val_loss: 0.4592\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.5006 - val_loss: 0.4550\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.4966 - val_loss: 0.4509\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.4929 - val_loss: 0.4472\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.4895 - val_loss: 0.4437\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.4862 - val_loss: 0.4400\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.4828 - val_loss: 0.4367\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.4799 - val_loss: 0.4337\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.4769 - val_loss: 0.4306\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.4741 - val_loss: 0.4278\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.4716 - val_loss: 0.4251\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.4690 - val_loss: 0.4225\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.4665 - val_loss: 0.4198\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.4640 - val_loss: 0.4175\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.4618 - val_loss: 0.4150\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.4596 - val_loss: 0.4128\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.4574 - val_loss: 0.4107\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.4554 - val_loss: 0.4086\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.4533 - val_loss: 0.4066\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.4513 - val_loss: 0.4046\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.4493 - val_loss: 0.4027\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.4475 - val_loss: 0.4008\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.4456 - val_loss: 0.3990\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.4439 - val_loss: 0.3973\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.4421 - val_loss: 0.3956\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.4404 - val_loss: 0.3939\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.4388 - val_loss: 0.3923\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.4371 - val_loss: 0.3907\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.4355 - val_loss: 0.3892\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.4339 - val_loss: 0.3876\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.4323 - val_loss: 0.3861\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.4308 - val_loss: 0.3846\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.4293 - val_loss: 0.3831\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.4277 - val_loss: 0.3816\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.4262 - val_loss: 0.3802\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.4248 - val_loss: 0.3788\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.4234 - val_loss: 0.3774\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.4219 - val_loss: 0.3760\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.4205 - val_loss: 0.3747\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.4191 - val_loss: 0.3733\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.4177 - val_loss: 0.3720\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.4164 - val_loss: 0.3707\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.4150 - val_loss: 0.3694\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.4138 - val_loss: 0.3681\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.4123 - val_loss: 0.3668\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.4110 - val_loss: 0.3656\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.4097 - val_loss: 0.3644\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.4085 - val_loss: 0.3631\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.4071 - val_loss: 0.3619\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.4058 - val_loss: 0.3606\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.4045 - val_loss: 0.3594\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.4037 - val_loss: 0.3582\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.4019 - val_loss: 0.3570\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.4007 - val_loss: 0.3559\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.3995 - val_loss: 0.3547\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.3983 - val_loss: 0.3535\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.3970 - val_loss: 0.3523\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.3957 - val_loss: 0.3512\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.3945 - val_loss: 0.3501\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.3932 - val_loss: 0.3489\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.3921 - val_loss: 0.3477\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.3908 - val_loss: 0.3466\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.3895 - val_loss: 0.3454\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.3883 - val_loss: 0.3443\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.3870 - val_loss: 0.3431\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.3859 - val_loss: 0.3419\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.3846 - val_loss: 0.3408\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.3833 - val_loss: 0.3396\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.3824 - val_loss: 0.3386\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.3809 - val_loss: 0.3374\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.3797 - val_loss: 0.3362\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.3785 - val_loss: 0.3350\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.3772 - val_loss: 0.3339\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.3763 - val_loss: 0.3327\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.3750 - val_loss: 0.3316\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.3737 - val_loss: 0.3305\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.3723 - val_loss: 0.3294\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.3712 - val_loss: 0.3282\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.3700 - val_loss: 0.3270\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.3687 - val_loss: 0.3259\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.3674 - val_loss: 0.3247\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.3663 - val_loss: 0.3236\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.3651 - val_loss: 0.3224\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.3639 - val_loss: 0.3214\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.3627 - val_loss: 0.3203\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.3617 - val_loss: 0.3190\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.3602 - val_loss: 0.3178\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.3589 - val_loss: 0.3167\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.3578 - val_loss: 0.3156\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.3565 - val_loss: 0.3144\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.3553 - val_loss: 0.3132\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.3540 - val_loss: 0.3121\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.3529 - val_loss: 0.3111\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.3516 - val_loss: 0.3098\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.3504 - val_loss: 0.3086\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.3491 - val_loss: 0.3076\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.3479 - val_loss: 0.3064\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.3468 - val_loss: 0.3051\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.3455 - val_loss: 0.3039\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.3442 - val_loss: 0.3028\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.3429 - val_loss: 0.3017\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.3425 - val_loss: 0.3009\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.3407 - val_loss: 0.2995\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.3393 - val_loss: 0.2982\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.3384 - val_loss: 0.2971\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.3369 - val_loss: 0.2958\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.3356 - val_loss: 0.2947\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.3349 - val_loss: 0.2935\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.3334 - val_loss: 0.2924\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.3319 - val_loss: 0.2914\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.3308 - val_loss: 0.2903\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.3297 - val_loss: 0.2892\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.3286 - val_loss: 0.2883\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.3276 - val_loss: 0.2869\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.3260 - val_loss: 0.2858\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.3252 - val_loss: 0.2845\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.3235 - val_loss: 0.2834\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.3224 - val_loss: 0.2823\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.3213 - val_loss: 0.2812\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.3202 - val_loss: 0.2800\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.3188 - val_loss: 0.2789\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.3175 - val_loss: 0.2779\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.3167 - val_loss: 0.2770\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.3155 - val_loss: 0.2757\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.3140 - val_loss: 0.2745\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.3128 - val_loss: 0.2733\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.3125 - val_loss: 0.2720\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.3107 - val_loss: 0.2709\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.3094 - val_loss: 0.2698\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.3080 - val_loss: 0.2689\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.3073 - val_loss: 0.2681\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.3064 - val_loss: 0.2672\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.3046 - val_loss: 0.2655\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.3035 - val_loss: 0.2642\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.3024 - val_loss: 0.2632\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.3012 - val_loss: 0.2621\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.3000 - val_loss: 0.2611\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.2991 - val_loss: 0.2601\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.2978 - val_loss: 0.2591\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.2966 - val_loss: 0.2579\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.2957 - val_loss: 0.2568\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.2943 - val_loss: 0.2559\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.2932 - val_loss: 0.2550\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.2921 - val_loss: 0.2539\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.2912 - val_loss: 0.2527\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.2902 - val_loss: 0.2519\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.2889 - val_loss: 0.2508\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.2876 - val_loss: 0.2495\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.2866 - val_loss: 0.2483\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.2855 - val_loss: 0.2474\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.2845 - val_loss: 0.2465\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.2833 - val_loss: 0.2455\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.2824 - val_loss: 0.2443\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.2814 - val_loss: 0.2432\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.2802 - val_loss: 0.2423\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.2791 - val_loss: 0.2416\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.2782 - val_loss: 0.2404\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.2771 - val_loss: 0.2394\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.2761 - val_loss: 0.2388\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.2751 - val_loss: 0.2379\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.2744 - val_loss: 0.2363\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.2729 - val_loss: 0.2353\n",
      "Epoch 225/1000\n",
      "0s - loss: 0.2720 - val_loss: 0.2346\n",
      "Epoch 226/1000\n",
      "0s - loss: 0.2707 - val_loss: 0.2338\n",
      "Epoch 227/1000\n",
      "0s - loss: 0.2700 - val_loss: 0.2330\n",
      "Epoch 228/1000\n",
      "0s - loss: 0.2689 - val_loss: 0.2315\n",
      "Epoch 229/1000\n",
      "0s - loss: 0.2677 - val_loss: 0.2305\n",
      "Epoch 230/1000\n",
      "0s - loss: 0.2668 - val_loss: 0.2296\n",
      "Epoch 231/1000\n",
      "0s - loss: 0.2660 - val_loss: 0.2290\n",
      "Epoch 232/1000\n",
      "0s - loss: 0.2646 - val_loss: 0.2281\n",
      "Epoch 233/1000\n",
      "0s - loss: 0.2636 - val_loss: 0.2270\n",
      "Epoch 234/1000\n",
      "0s - loss: 0.2628 - val_loss: 0.2259\n",
      "Epoch 235/1000\n",
      "0s - loss: 0.2620 - val_loss: 0.2250\n",
      "Epoch 236/1000\n",
      "0s - loss: 0.2611 - val_loss: 0.2245\n",
      "Epoch 237/1000\n",
      "0s - loss: 0.2598 - val_loss: 0.2233\n",
      "Epoch 238/1000\n",
      "0s - loss: 0.2590 - val_loss: 0.2225\n",
      "Epoch 239/1000\n",
      "0s - loss: 0.2579 - val_loss: 0.2213\n",
      "Epoch 240/1000\n",
      "0s - loss: 0.2570 - val_loss: 0.2206\n",
      "Epoch 241/1000\n",
      "0s - loss: 0.2561 - val_loss: 0.2199\n",
      "Epoch 242/1000\n",
      "0s - loss: 0.2551 - val_loss: 0.2188\n",
      "Epoch 243/1000\n",
      "0s - loss: 0.2542 - val_loss: 0.2177\n",
      "Epoch 244/1000\n",
      "0s - loss: 0.2532 - val_loss: 0.2169\n",
      "Epoch 245/1000\n",
      "0s - loss: 0.2529 - val_loss: 0.2164\n",
      "Epoch 246/1000\n",
      "0s - loss: 0.2512 - val_loss: 0.2154\n",
      "Epoch 247/1000\n",
      "0s - loss: 0.2506 - val_loss: 0.2143\n",
      "Epoch 248/1000\n",
      "0s - loss: 0.2493 - val_loss: 0.2135\n",
      "Epoch 249/1000\n",
      "0s - loss: 0.2485 - val_loss: 0.2129\n",
      "Epoch 250/1000\n",
      "0s - loss: 0.2481 - val_loss: 0.2124\n",
      "Epoch 251/1000\n",
      "0s - loss: 0.2468 - val_loss: 0.2113\n",
      "Epoch 252/1000\n",
      "0s - loss: 0.2463 - val_loss: 0.2100\n",
      "Epoch 253/1000\n",
      "0s - loss: 0.2454 - val_loss: 0.2093\n",
      "Epoch 254/1000\n",
      "0s - loss: 0.2441 - val_loss: 0.2085\n",
      "Epoch 255/1000\n",
      "0s - loss: 0.2433 - val_loss: 0.2077\n",
      "Epoch 256/1000\n",
      "0s - loss: 0.2423 - val_loss: 0.2069\n",
      "Epoch 257/1000\n",
      "0s - loss: 0.2416 - val_loss: 0.2059\n",
      "Epoch 258/1000\n",
      "0s - loss: 0.2409 - val_loss: 0.2050\n",
      "Epoch 259/1000\n",
      "0s - loss: 0.2400 - val_loss: 0.2043\n",
      "Epoch 260/1000\n",
      "0s - loss: 0.2388 - val_loss: 0.2037\n",
      "Epoch 261/1000\n",
      "0s - loss: 0.2380 - val_loss: 0.2031\n",
      "Epoch 262/1000\n",
      "0s - loss: 0.2373 - val_loss: 0.2024\n",
      "Epoch 263/1000\n",
      "0s - loss: 0.2363 - val_loss: 0.2014\n",
      "Epoch 264/1000\n",
      "0s - loss: 0.2356 - val_loss: 0.2005\n",
      "Epoch 265/1000\n",
      "0s - loss: 0.2346 - val_loss: 0.1997\n",
      "Epoch 266/1000\n",
      "0s - loss: 0.2339 - val_loss: 0.1991\n",
      "Epoch 267/1000\n",
      "0s - loss: 0.2334 - val_loss: 0.1989\n",
      "Epoch 268/1000\n",
      "0s - loss: 0.2325 - val_loss: 0.1976\n",
      "Epoch 269/1000\n",
      "0s - loss: 0.2314 - val_loss: 0.1967\n",
      "Epoch 270/1000\n",
      "0s - loss: 0.2306 - val_loss: 0.1958\n",
      "Epoch 271/1000\n",
      "0s - loss: 0.2300 - val_loss: 0.1949\n",
      "Epoch 272/1000\n",
      "0s - loss: 0.2291 - val_loss: 0.1945\n",
      "Epoch 273/1000\n",
      "0s - loss: 0.2283 - val_loss: 0.1940\n",
      "Epoch 274/1000\n",
      "0s - loss: 0.2279 - val_loss: 0.1929\n",
      "Epoch 275/1000\n",
      "0s - loss: 0.2268 - val_loss: 0.1923\n",
      "Epoch 276/1000\n",
      "0s - loss: 0.2258 - val_loss: 0.1917\n",
      "Epoch 277/1000\n",
      "0s - loss: 0.2252 - val_loss: 0.1909\n",
      "Epoch 278/1000\n",
      "0s - loss: 0.2244 - val_loss: 0.1900\n",
      "Epoch 279/1000\n",
      "0s - loss: 0.2235 - val_loss: 0.1892\n",
      "Epoch 280/1000\n",
      "0s - loss: 0.2230 - val_loss: 0.1885\n",
      "Epoch 281/1000\n",
      "0s - loss: 0.2221 - val_loss: 0.1883\n",
      "Epoch 282/1000\n",
      "0s - loss: 0.2213 - val_loss: 0.1877\n",
      "Epoch 283/1000\n",
      "0s - loss: 0.2205 - val_loss: 0.1866\n",
      "Epoch 284/1000\n",
      "0s - loss: 0.2198 - val_loss: 0.1857\n",
      "Epoch 285/1000\n",
      "0s - loss: 0.2189 - val_loss: 0.1851\n",
      "Epoch 286/1000\n",
      "0s - loss: 0.2182 - val_loss: 0.1844\n",
      "Epoch 287/1000\n",
      "0s - loss: 0.2179 - val_loss: 0.1835\n",
      "Epoch 288/1000\n",
      "0s - loss: 0.2169 - val_loss: 0.1832\n",
      "Epoch 289/1000\n",
      "0s - loss: 0.2167 - val_loss: 0.1822\n",
      "Epoch 290/1000\n",
      "0s - loss: 0.2159 - val_loss: 0.1822\n",
      "Epoch 291/1000\n",
      "0s - loss: 0.2151 - val_loss: 0.1812\n",
      "Epoch 292/1000\n",
      "0s - loss: 0.2144 - val_loss: 0.1809\n",
      "Epoch 293/1000\n",
      "0s - loss: 0.2133 - val_loss: 0.1795\n",
      "Epoch 294/1000\n",
      "0s - loss: 0.2130 - val_loss: 0.1787\n",
      "Epoch 295/1000\n",
      "0s - loss: 0.2118 - val_loss: 0.1781\n",
      "Epoch 296/1000\n",
      "0s - loss: 0.2114 - val_loss: 0.1782\n",
      "Epoch 297/1000\n",
      "0s - loss: 0.2104 - val_loss: 0.1773\n",
      "Epoch 298/1000\n",
      "0s - loss: 0.2097 - val_loss: 0.1764\n",
      "Epoch 299/1000\n",
      "0s - loss: 0.2090 - val_loss: 0.1758\n",
      "Epoch 300/1000\n",
      "0s - loss: 0.2083 - val_loss: 0.1750\n",
      "Epoch 301/1000\n",
      "0s - loss: 0.2075 - val_loss: 0.1743\n",
      "Epoch 302/1000\n",
      "0s - loss: 0.2068 - val_loss: 0.1736\n",
      "Epoch 303/1000\n",
      "0s - loss: 0.2068 - val_loss: 0.1731\n",
      "Epoch 304/1000\n",
      "0s - loss: 0.2055 - val_loss: 0.1724\n",
      "Epoch 305/1000\n",
      "0s - loss: 0.2048 - val_loss: 0.1718\n",
      "Epoch 306/1000\n",
      "0s - loss: 0.2043 - val_loss: 0.1715\n",
      "Epoch 307/1000\n",
      "0s - loss: 0.2039 - val_loss: 0.1706\n",
      "Epoch 308/1000\n",
      "0s - loss: 0.2028 - val_loss: 0.1701\n",
      "Epoch 309/1000\n",
      "0s - loss: 0.2027 - val_loss: 0.1705\n",
      "Epoch 310/1000\n",
      "0s - loss: 0.2020 - val_loss: 0.1696\n",
      "Epoch 311/1000\n",
      "0s - loss: 0.2012 - val_loss: 0.1682\n",
      "Epoch 312/1000\n",
      "0s - loss: 0.2001 - val_loss: 0.1675\n",
      "Epoch 313/1000\n",
      "0s - loss: 0.1998 - val_loss: 0.1668\n",
      "Epoch 314/1000\n",
      "0s - loss: 0.1997 - val_loss: 0.1668\n",
      "Epoch 315/1000\n",
      "0s - loss: 0.1986 - val_loss: 0.1659\n",
      "Epoch 316/1000\n",
      "0s - loss: 0.1977 - val_loss: 0.1653\n",
      "Epoch 317/1000\n",
      "0s - loss: 0.1973 - val_loss: 0.1650\n",
      "Epoch 318/1000\n",
      "0s - loss: 0.1966 - val_loss: 0.1644\n",
      "Epoch 319/1000\n",
      "0s - loss: 0.1961 - val_loss: 0.1633\n",
      "Epoch 320/1000\n",
      "0s - loss: 0.1956 - val_loss: 0.1627\n",
      "Epoch 321/1000\n",
      "0s - loss: 0.1960 - val_loss: 0.1628\n",
      "Epoch 322/1000\n",
      "0s - loss: 0.1941 - val_loss: 0.1622\n",
      "Epoch 323/1000\n",
      "0s - loss: 0.1933 - val_loss: 0.1614\n",
      "Epoch 324/1000\n",
      "0s - loss: 0.1926 - val_loss: 0.1606\n",
      "Epoch 325/1000\n",
      "0s - loss: 0.1923 - val_loss: 0.1599\n",
      "Epoch 326/1000\n",
      "0s - loss: 0.1917 - val_loss: 0.1595\n",
      "Epoch 327/1000\n",
      "0s - loss: 0.1911 - val_loss: 0.1594\n",
      "Epoch 328/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.1903 - val_loss: 0.1586\n",
      "Epoch 329/1000\n",
      "0s - loss: 0.1898 - val_loss: 0.1578\n",
      "Epoch 330/1000\n",
      "0s - loss: 0.1891 - val_loss: 0.1573\n",
      "Epoch 331/1000\n",
      "0s - loss: 0.1885 - val_loss: 0.1568\n",
      "Epoch 332/1000\n",
      "0s - loss: 0.1879 - val_loss: 0.1563\n",
      "Epoch 333/1000\n",
      "0s - loss: 0.1874 - val_loss: 0.1561\n",
      "Epoch 334/1000\n",
      "0s - loss: 0.1874 - val_loss: 0.1560\n",
      "Epoch 335/1000\n",
      "0s - loss: 0.1869 - val_loss: 0.1546\n",
      "Epoch 336/1000\n",
      "0s - loss: 0.1860 - val_loss: 0.1542\n",
      "Epoch 337/1000\n",
      "0s - loss: 0.1852 - val_loss: 0.1534\n",
      "Epoch 338/1000\n",
      "0s - loss: 0.1846 - val_loss: 0.1530\n",
      "Epoch 339/1000\n",
      "0s - loss: 0.1840 - val_loss: 0.1526\n",
      "Epoch 340/1000\n",
      "0s - loss: 0.1836 - val_loss: 0.1520\n",
      "Epoch 341/1000\n",
      "0s - loss: 0.1828 - val_loss: 0.1519\n",
      "Epoch 342/1000\n",
      "0s - loss: 0.1823 - val_loss: 0.1514\n",
      "Epoch 343/1000\n",
      "0s - loss: 0.1819 - val_loss: 0.1508\n",
      "Epoch 344/1000\n",
      "0s - loss: 0.1812 - val_loss: 0.1502\n",
      "Epoch 345/1000\n",
      "0s - loss: 0.1814 - val_loss: 0.1493\n",
      "Epoch 346/1000\n",
      "0s - loss: 0.1801 - val_loss: 0.1489\n",
      "Epoch 347/1000\n",
      "0s - loss: 0.1795 - val_loss: 0.1486\n",
      "Epoch 348/1000\n",
      "0s - loss: 0.1793 - val_loss: 0.1484\n",
      "Epoch 349/1000\n",
      "0s - loss: 0.1792 - val_loss: 0.1474\n",
      "Epoch 350/1000\n",
      "0s - loss: 0.1780 - val_loss: 0.1471\n",
      "Epoch 351/1000\n",
      "0s - loss: 0.1774 - val_loss: 0.1466\n",
      "Epoch 352/1000\n",
      "0s - loss: 0.1771 - val_loss: 0.1460\n",
      "Epoch 353/1000\n",
      "0s - loss: 0.1776 - val_loss: 0.1461\n",
      "Epoch 354/1000\n",
      "0s - loss: 0.1760 - val_loss: 0.1451\n",
      "Epoch 355/1000\n",
      "0s - loss: 0.1755 - val_loss: 0.1444\n",
      "Epoch 356/1000\n",
      "0s - loss: 0.1750 - val_loss: 0.1440\n",
      "Epoch 357/1000\n",
      "0s - loss: 0.1745 - val_loss: 0.1436\n",
      "Epoch 358/1000\n",
      "0s - loss: 0.1740 - val_loss: 0.1431\n",
      "Epoch 359/1000\n",
      "0s - loss: 0.1737 - val_loss: 0.1425\n",
      "Epoch 360/1000\n",
      "0s - loss: 0.1728 - val_loss: 0.1422\n",
      "Epoch 361/1000\n",
      "0s - loss: 0.1723 - val_loss: 0.1418\n",
      "Epoch 362/1000\n",
      "0s - loss: 0.1718 - val_loss: 0.1417\n",
      "Epoch 363/1000\n",
      "0s - loss: 0.1718 - val_loss: 0.1416\n",
      "Epoch 364/1000\n",
      "0s - loss: 0.1714 - val_loss: 0.1403\n",
      "Epoch 365/1000\n",
      "0s - loss: 0.1704 - val_loss: 0.1399\n",
      "Epoch 366/1000\n",
      "0s - loss: 0.1704 - val_loss: 0.1393\n",
      "Epoch 367/1000\n",
      "0s - loss: 0.1695 - val_loss: 0.1390\n",
      "Epoch 368/1000\n",
      "0s - loss: 0.1688 - val_loss: 0.1386\n",
      "Epoch 369/1000\n",
      "0s - loss: 0.1682 - val_loss: 0.1383\n",
      "Epoch 370/1000\n",
      "0s - loss: 0.1683 - val_loss: 0.1381\n",
      "Epoch 371/1000\n",
      "0s - loss: 0.1674 - val_loss: 0.1373\n",
      "Epoch 372/1000\n",
      "0s - loss: 0.1670 - val_loss: 0.1367\n",
      "Epoch 373/1000\n",
      "0s - loss: 0.1665 - val_loss: 0.1362\n",
      "Epoch 374/1000\n",
      "0s - loss: 0.1666 - val_loss: 0.1363\n",
      "Epoch 375/1000\n",
      "0s - loss: 0.1655 - val_loss: 0.1356\n",
      "Epoch 376/1000\n",
      "0s - loss: 0.1652 - val_loss: 0.1349\n",
      "Epoch 377/1000\n",
      "0s - loss: 0.1645 - val_loss: 0.1345\n",
      "Epoch 378/1000\n",
      "0s - loss: 0.1642 - val_loss: 0.1342\n",
      "Epoch 379/1000\n",
      "0s - loss: 0.1637 - val_loss: 0.1340\n",
      "Epoch 380/1000\n",
      "0s - loss: 0.1631 - val_loss: 0.1334\n",
      "Epoch 381/1000\n",
      "0s - loss: 0.1627 - val_loss: 0.1329\n",
      "Epoch 382/1000\n",
      "0s - loss: 0.1625 - val_loss: 0.1325\n",
      "Epoch 383/1000\n",
      "0s - loss: 0.1617 - val_loss: 0.1325\n",
      "Epoch 384/1000\n",
      "0s - loss: 0.1613 - val_loss: 0.1322\n",
      "Epoch 385/1000\n",
      "0s - loss: 0.1612 - val_loss: 0.1317\n",
      "Epoch 386/1000\n",
      "0s - loss: 0.1602 - val_loss: 0.1309\n",
      "Epoch 387/1000\n",
      "0s - loss: 0.1604 - val_loss: 0.1303\n",
      "Epoch 388/1000\n",
      "0s - loss: 0.1605 - val_loss: 0.1299\n",
      "Epoch 389/1000\n",
      "0s - loss: 0.1590 - val_loss: 0.1299\n",
      "Epoch 390/1000\n",
      "0s - loss: 0.1588 - val_loss: 0.1301\n",
      "Epoch 391/1000\n",
      "0s - loss: 0.1584 - val_loss: 0.1293\n",
      "Epoch 392/1000\n",
      "0s - loss: 0.1578 - val_loss: 0.1284\n",
      "Epoch 393/1000\n",
      "0s - loss: 0.1578 - val_loss: 0.1279\n",
      "Epoch 394/1000\n",
      "0s - loss: 0.1571 - val_loss: 0.1277\n",
      "Epoch 395/1000\n",
      "0s - loss: 0.1564 - val_loss: 0.1273\n",
      "Epoch 396/1000\n",
      "0s - loss: 0.1562 - val_loss: 0.1271\n",
      "Epoch 397/1000\n",
      "0s - loss: 0.1558 - val_loss: 0.1266\n",
      "Epoch 398/1000\n",
      "0s - loss: 0.1555 - val_loss: 0.1266\n",
      "Epoch 399/1000\n",
      "0s - loss: 0.1552 - val_loss: 0.1260\n",
      "Epoch 400/1000\n",
      "0s - loss: 0.1546 - val_loss: 0.1256\n",
      "Epoch 401/1000\n",
      "0s - loss: 0.1539 - val_loss: 0.1251\n",
      "Epoch 402/1000\n",
      "0s - loss: 0.1538 - val_loss: 0.1245\n",
      "Epoch 403/1000\n",
      "0s - loss: 0.1533 - val_loss: 0.1243\n",
      "Epoch 404/1000\n",
      "0s - loss: 0.1530 - val_loss: 0.1239\n",
      "Epoch 405/1000\n",
      "0s - loss: 0.1526 - val_loss: 0.1241\n",
      "Epoch 406/1000\n",
      "0s - loss: 0.1520 - val_loss: 0.1235\n",
      "Epoch 407/1000\n",
      "0s - loss: 0.1515 - val_loss: 0.1229\n",
      "Epoch 408/1000\n",
      "0s - loss: 0.1514 - val_loss: 0.1223\n",
      "Epoch 409/1000\n",
      "0s - loss: 0.1509 - val_loss: 0.1219\n",
      "Epoch 410/1000\n",
      "0s - loss: 0.1505 - val_loss: 0.1218\n",
      "Epoch 411/1000\n",
      "0s - loss: 0.1498 - val_loss: 0.1216\n",
      "Epoch 412/1000\n",
      "0s - loss: 0.1496 - val_loss: 0.1215\n",
      "Epoch 413/1000\n",
      "0s - loss: 0.1496 - val_loss: 0.1212\n",
      "Epoch 414/1000\n",
      "0s - loss: 0.1496 - val_loss: 0.1202\n",
      "Epoch 415/1000\n",
      "0s - loss: 0.1491 - val_loss: 0.1199\n",
      "Epoch 416/1000\n",
      "0s - loss: 0.1478 - val_loss: 0.1194\n",
      "Epoch 417/1000\n",
      "0s - loss: 0.1478 - val_loss: 0.1191\n",
      "Epoch 418/1000\n",
      "0s - loss: 0.1476 - val_loss: 0.1188\n",
      "Epoch 419/1000\n",
      "0s - loss: 0.1467 - val_loss: 0.1185\n",
      "Epoch 420/1000\n",
      "0s - loss: 0.1469 - val_loss: 0.1184\n",
      "Epoch 421/1000\n",
      "0s - loss: 0.1459 - val_loss: 0.1177\n",
      "Epoch 422/1000\n",
      "0s - loss: 0.1457 - val_loss: 0.1174\n",
      "Epoch 423/1000\n",
      "0s - loss: 0.1453 - val_loss: 0.1170\n",
      "Epoch 424/1000\n",
      "0s - loss: 0.1451 - val_loss: 0.1167\n",
      "Epoch 425/1000\n",
      "0s - loss: 0.1447 - val_loss: 0.1164\n",
      "Epoch 426/1000\n",
      "0s - loss: 0.1443 - val_loss: 0.1161\n",
      "Epoch 427/1000\n",
      "0s - loss: 0.1438 - val_loss: 0.1159\n",
      "Epoch 428/1000\n",
      "0s - loss: 0.1436 - val_loss: 0.1154\n",
      "Epoch 429/1000\n",
      "0s - loss: 0.1433 - val_loss: 0.1153\n",
      "Epoch 430/1000\n",
      "0s - loss: 0.1435 - val_loss: 0.1146\n",
      "Epoch 431/1000\n",
      "0s - loss: 0.1425 - val_loss: 0.1144\n",
      "Epoch 432/1000\n",
      "0s - loss: 0.1420 - val_loss: 0.1141\n",
      "Epoch 433/1000\n",
      "0s - loss: 0.1415 - val_loss: 0.1138\n",
      "Epoch 434/1000\n",
      "0s - loss: 0.1412 - val_loss: 0.1135\n",
      "Epoch 435/1000\n",
      "0s - loss: 0.1411 - val_loss: 0.1135\n",
      "Epoch 436/1000\n",
      "0s - loss: 0.1405 - val_loss: 0.1129\n",
      "Epoch 437/1000\n",
      "0s - loss: 0.1402 - val_loss: 0.1124\n",
      "Epoch 438/1000\n",
      "0s - loss: 0.1399 - val_loss: 0.1120\n",
      "Epoch 439/1000\n",
      "0s - loss: 0.1395 - val_loss: 0.1117\n",
      "Epoch 440/1000\n",
      "0s - loss: 0.1393 - val_loss: 0.1116\n",
      "Epoch 441/1000\n",
      "0s - loss: 0.1390 - val_loss: 0.1115\n",
      "Epoch 442/1000\n",
      "0s - loss: 0.1387 - val_loss: 0.1108\n",
      "Epoch 443/1000\n",
      "0s - loss: 0.1381 - val_loss: 0.1105\n",
      "Epoch 444/1000\n",
      "0s - loss: 0.1377 - val_loss: 0.1102\n",
      "Epoch 445/1000\n",
      "0s - loss: 0.1373 - val_loss: 0.1100\n",
      "Epoch 446/1000\n",
      "0s - loss: 0.1370 - val_loss: 0.1095\n",
      "Epoch 447/1000\n",
      "0s - loss: 0.1370 - val_loss: 0.1092\n",
      "Epoch 448/1000\n",
      "0s - loss: 0.1362 - val_loss: 0.1090\n",
      "Epoch 449/1000\n",
      "0s - loss: 0.1359 - val_loss: 0.1090\n",
      "Epoch 450/1000\n",
      "0s - loss: 0.1357 - val_loss: 0.1087\n",
      "Epoch 451/1000\n",
      "0s - loss: 0.1353 - val_loss: 0.1082\n",
      "Epoch 452/1000\n",
      "0s - loss: 0.1351 - val_loss: 0.1077\n",
      "Epoch 453/1000\n",
      "0s - loss: 0.1348 - val_loss: 0.1074\n",
      "Epoch 454/1000\n",
      "0s - loss: 0.1346 - val_loss: 0.1073\n",
      "Epoch 455/1000\n",
      "0s - loss: 0.1341 - val_loss: 0.1071\n",
      "Epoch 456/1000\n",
      "0s - loss: 0.1340 - val_loss: 0.1065\n",
      "Epoch 457/1000\n",
      "0s - loss: 0.1334 - val_loss: 0.1061\n",
      "Epoch 458/1000\n",
      "0s - loss: 0.1330 - val_loss: 0.1059\n",
      "Epoch 459/1000\n",
      "0s - loss: 0.1327 - val_loss: 0.1057\n",
      "Epoch 460/1000\n",
      "0s - loss: 0.1323 - val_loss: 0.1054\n",
      "Epoch 461/1000\n",
      "0s - loss: 0.1320 - val_loss: 0.1051\n",
      "Epoch 462/1000\n",
      "0s - loss: 0.1319 - val_loss: 0.1047\n",
      "Epoch 463/1000\n",
      "0s - loss: 0.1315 - val_loss: 0.1044\n",
      "Epoch 464/1000\n",
      "0s - loss: 0.1312 - val_loss: 0.1042\n",
      "Epoch 465/1000\n",
      "0s - loss: 0.1307 - val_loss: 0.1041\n",
      "Epoch 466/1000\n",
      "0s - loss: 0.1307 - val_loss: 0.1038\n",
      "Epoch 467/1000\n",
      "0s - loss: 0.1302 - val_loss: 0.1033\n",
      "Epoch 468/1000\n",
      "0s - loss: 0.1300 - val_loss: 0.1031\n",
      "Epoch 469/1000\n",
      "0s - loss: 0.1296 - val_loss: 0.1027\n",
      "Epoch 470/1000\n",
      "0s - loss: 0.1293 - val_loss: 0.1024\n",
      "Epoch 471/1000\n",
      "0s - loss: 0.1289 - val_loss: 0.1022\n",
      "Epoch 472/1000\n",
      "0s - loss: 0.1285 - val_loss: 0.1020\n",
      "Epoch 473/1000\n",
      "0s - loss: 0.1283 - val_loss: 0.1017\n",
      "Epoch 474/1000\n",
      "0s - loss: 0.1284 - val_loss: 0.1015\n",
      "Epoch 475/1000\n",
      "0s - loss: 0.1277 - val_loss: 0.1012\n",
      "Epoch 476/1000\n",
      "0s - loss: 0.1274 - val_loss: 0.1008\n",
      "Epoch 477/1000\n",
      "0s - loss: 0.1270 - val_loss: 0.1006\n",
      "Epoch 478/1000\n",
      "0s - loss: 0.1267 - val_loss: 0.1003\n",
      "Epoch 479/1000\n",
      "0s - loss: 0.1265 - val_loss: 0.1000\n",
      "Epoch 480/1000\n",
      "0s - loss: 0.1262 - val_loss: 0.0998\n",
      "Epoch 481/1000\n",
      "0s - loss: 0.1265 - val_loss: 0.0994\n",
      "Epoch 482/1000\n",
      "0s - loss: 0.1256 - val_loss: 0.0992\n",
      "Epoch 483/1000\n",
      "0s - loss: 0.1267 - val_loss: 0.0994\n",
      "Epoch 484/1000\n",
      "0s - loss: 0.1251 - val_loss: 0.0989\n",
      "Epoch 485/1000\n",
      "0s - loss: 0.1246 - val_loss: 0.0984\n",
      "Epoch 486/1000\n",
      "0s - loss: 0.1243 - val_loss: 0.0981\n",
      "Epoch 487/1000\n",
      "0s - loss: 0.1249 - val_loss: 0.0980\n",
      "Epoch 488/1000\n",
      "0s - loss: 0.1243 - val_loss: 0.0976\n",
      "Epoch 489/1000\n",
      "0s - loss: 0.1235 - val_loss: 0.0974\n",
      "Epoch 490/1000\n",
      "0s - loss: 0.1232 - val_loss: 0.0972\n",
      "Epoch 491/1000\n",
      "0s - loss: 0.1231 - val_loss: 0.0970\n",
      "Epoch 492/1000\n",
      "0s - loss: 0.1235 - val_loss: 0.0968\n",
      "Epoch 493/1000\n",
      "0s - loss: 0.1223 - val_loss: 0.0963\n",
      "Epoch 494/1000\n",
      "0s - loss: 0.1224 - val_loss: 0.0961\n",
      "Epoch 495/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.1220 - val_loss: 0.0958\n",
      "Epoch 496/1000\n",
      "0s - loss: 0.1217 - val_loss: 0.0956\n",
      "Epoch 497/1000\n",
      "0s - loss: 0.1215 - val_loss: 0.0955\n",
      "Epoch 498/1000\n",
      "0s - loss: 0.1213 - val_loss: 0.0953\n",
      "Epoch 499/1000\n",
      "0s - loss: 0.1218 - val_loss: 0.0948\n",
      "Epoch 500/1000\n",
      "0s - loss: 0.1208 - val_loss: 0.0946\n",
      "Epoch 501/1000\n",
      "0s - loss: 0.1205 - val_loss: 0.0944\n",
      "Epoch 502/1000\n",
      "0s - loss: 0.1199 - val_loss: 0.0942\n",
      "Epoch 503/1000\n",
      "0s - loss: 0.1197 - val_loss: 0.0939\n",
      "Epoch 504/1000\n",
      "0s - loss: 0.1194 - val_loss: 0.0937\n",
      "Epoch 505/1000\n",
      "0s - loss: 0.1192 - val_loss: 0.0935\n",
      "Epoch 506/1000\n",
      "0s - loss: 0.1189 - val_loss: 0.0931\n",
      "Epoch 507/1000\n",
      "0s - loss: 0.1186 - val_loss: 0.0929\n",
      "Epoch 508/1000\n",
      "0s - loss: 0.1185 - val_loss: 0.0926\n",
      "Epoch 509/1000\n",
      "0s - loss: 0.1179 - val_loss: 0.0924\n",
      "Epoch 510/1000\n",
      "0s - loss: 0.1177 - val_loss: 0.0923\n",
      "Epoch 511/1000\n",
      "0s - loss: 0.1175 - val_loss: 0.0921\n",
      "Epoch 512/1000\n",
      "0s - loss: 0.1180 - val_loss: 0.0920\n",
      "Epoch 513/1000\n",
      "0s - loss: 0.1171 - val_loss: 0.0916\n",
      "Epoch 514/1000\n",
      "0s - loss: 0.1176 - val_loss: 0.0912\n",
      "Epoch 515/1000\n",
      "0s - loss: 0.1166 - val_loss: 0.0910\n",
      "Epoch 516/1000\n",
      "0s - loss: 0.1170 - val_loss: 0.0908\n",
      "Epoch 517/1000\n",
      "0s - loss: 0.1161 - val_loss: 0.0908\n",
      "Epoch 518/1000\n",
      "0s - loss: 0.1168 - val_loss: 0.0913\n",
      "Epoch 519/1000\n",
      "0s - loss: 0.1163 - val_loss: 0.0902\n",
      "Epoch 520/1000\n",
      "0s - loss: 0.1152 - val_loss: 0.0898\n",
      "Epoch 521/1000\n",
      "0s - loss: 0.1159 - val_loss: 0.0897\n",
      "Epoch 522/1000\n",
      "0s - loss: 0.1149 - val_loss: 0.0894\n",
      "Epoch 523/1000\n",
      "0s - loss: 0.1155 - val_loss: 0.0894\n",
      "Epoch 524/1000\n",
      "0s - loss: 0.1146 - val_loss: 0.0889\n",
      "Epoch 525/1000\n",
      "0s - loss: 0.1141 - val_loss: 0.0888\n",
      "Epoch 526/1000\n",
      "0s - loss: 0.1139 - val_loss: 0.0885\n",
      "Epoch 527/1000\n",
      "0s - loss: 0.1135 - val_loss: 0.0884\n",
      "Epoch 528/1000\n",
      "0s - loss: 0.1139 - val_loss: 0.0880\n",
      "Epoch 529/1000\n",
      "0s - loss: 0.1130 - val_loss: 0.0879\n",
      "Epoch 530/1000\n",
      "0s - loss: 0.1128 - val_loss: 0.0877\n",
      "Epoch 531/1000\n",
      "0s - loss: 0.1126 - val_loss: 0.0874\n",
      "Epoch 532/1000\n",
      "0s - loss: 0.1123 - val_loss: 0.0872\n",
      "Epoch 533/1000\n",
      "0s - loss: 0.1125 - val_loss: 0.0871\n",
      "Epoch 534/1000\n",
      "0s - loss: 0.1121 - val_loss: 0.0870\n",
      "Epoch 535/1000\n",
      "0s - loss: 0.1119 - val_loss: 0.0865\n",
      "Epoch 536/1000\n",
      "0s - loss: 0.1117 - val_loss: 0.0864\n",
      "Epoch 537/1000\n",
      "0s - loss: 0.1120 - val_loss: 0.0862\n",
      "Epoch 538/1000\n",
      "0s - loss: 0.1109 - val_loss: 0.0859\n",
      "Epoch 539/1000\n",
      "0s - loss: 0.1108 - val_loss: 0.0858\n",
      "Epoch 540/1000\n",
      "0s - loss: 0.1105 - val_loss: 0.0856\n",
      "Epoch 541/1000\n",
      "0s - loss: 0.1104 - val_loss: 0.0854\n",
      "Epoch 542/1000\n",
      "0s - loss: 0.1100 - val_loss: 0.0851\n",
      "Epoch 543/1000\n",
      "0s - loss: 0.1097 - val_loss: 0.0850\n",
      "Epoch 544/1000\n",
      "0s - loss: 0.1098 - val_loss: 0.0847\n",
      "Epoch 545/1000\n",
      "0s - loss: 0.1092 - val_loss: 0.0846\n",
      "Epoch 546/1000\n",
      "0s - loss: 0.1092 - val_loss: 0.0844\n",
      "Epoch 547/1000\n",
      "0s - loss: 0.1088 - val_loss: 0.0842\n",
      "Epoch 548/1000\n",
      "0s - loss: 0.1087 - val_loss: 0.0839\n",
      "Epoch 549/1000\n",
      "0s - loss: 0.1086 - val_loss: 0.0837\n",
      "Epoch 550/1000\n",
      "0s - loss: 0.1085 - val_loss: 0.0835\n",
      "Epoch 551/1000\n",
      "0s - loss: 0.1083 - val_loss: 0.0833\n",
      "Epoch 552/1000\n",
      "0s - loss: 0.1077 - val_loss: 0.0831\n",
      "Epoch 553/1000\n",
      "0s - loss: 0.1078 - val_loss: 0.0833\n",
      "Epoch 554/1000\n",
      "0s - loss: 0.1078 - val_loss: 0.0831\n",
      "Epoch 555/1000\n",
      "0s - loss: 0.1074 - val_loss: 0.0827\n",
      "Epoch 556/1000\n",
      "0s - loss: 0.1062 - val_loss: 0.0824\n",
      "Epoch 557/1000\n",
      "0s - loss: 0.1070 - val_loss: 0.0826\n",
      "Epoch 558/1000\n",
      "0s - loss: 0.1072 - val_loss: 0.0823\n",
      "Epoch 00557: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1237c5978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "x,y = to_xy(df,\"species\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1) # raw probabilities to chosen class (highest probability)\n",
    "y_compare = np.argmax(y_test,axis=1) \n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "print(\"Accuracy score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      "0s - loss: 316.7480 - val_loss: 143.9322\n",
      "Epoch 2/1000\n",
      "0s - loss: 170.2959 - val_loss: 146.2024\n",
      "Epoch 3/1000\n",
      "0s - loss: 169.1210 - val_loss: 130.4604\n",
      "Epoch 4/1000\n",
      "0s - loss: 160.8918 - val_loss: 129.4031\n",
      "Epoch 5/1000\n",
      "0s - loss: 158.2303 - val_loss: 125.9698\n",
      "Epoch 6/1000\n",
      "0s - loss: 155.5611 - val_loss: 123.6084\n",
      "Epoch 7/1000\n",
      "0s - loss: 151.9272 - val_loss: 120.3858\n",
      "Epoch 8/1000\n",
      "0s - loss: 149.5229 - val_loss: 117.4196\n",
      "Epoch 9/1000\n",
      "0s - loss: 144.9586 - val_loss: 114.4553\n",
      "Epoch 10/1000\n",
      "0s - loss: 141.0111 - val_loss: 110.5800\n",
      "Epoch 11/1000\n",
      "0s - loss: 137.4179 - val_loss: 106.6368\n",
      "Epoch 12/1000\n",
      "0s - loss: 132.3896 - val_loss: 103.1701\n",
      "Epoch 13/1000\n",
      "0s - loss: 128.8498 - val_loss: 98.2748\n",
      "Epoch 14/1000\n",
      "0s - loss: 122.4264 - val_loss: 93.9718\n",
      "Epoch 15/1000\n",
      "0s - loss: 116.0715 - val_loss: 88.6000\n",
      "Epoch 16/1000\n",
      "0s - loss: 110.4475 - val_loss: 83.5048\n",
      "Epoch 17/1000\n",
      "0s - loss: 104.1281 - val_loss: 78.3910\n",
      "Epoch 18/1000\n",
      "0s - loss: 98.5318 - val_loss: 73.2276\n",
      "Epoch 19/1000\n",
      "0s - loss: 91.1450 - val_loss: 68.4807\n",
      "Epoch 20/1000\n",
      "0s - loss: 85.5373 - val_loss: 65.1744\n",
      "Epoch 21/1000\n",
      "0s - loss: 80.1935 - val_loss: 59.0533\n",
      "Epoch 22/1000\n",
      "0s - loss: 73.6845 - val_loss: 54.7919\n",
      "Epoch 23/1000\n",
      "0s - loss: 68.6347 - val_loss: 50.8517\n",
      "Epoch 24/1000\n",
      "0s - loss: 63.7009 - val_loss: 48.1600\n",
      "Epoch 25/1000\n",
      "0s - loss: 59.8472 - val_loss: 44.6539\n",
      "Epoch 26/1000\n",
      "0s - loss: 55.8575 - val_loss: 42.7775\n",
      "Epoch 27/1000\n",
      "0s - loss: 52.5258 - val_loss: 40.1870\n",
      "Epoch 28/1000\n",
      "0s - loss: 49.4442 - val_loss: 38.6708\n",
      "Epoch 29/1000\n",
      "0s - loss: 47.0751 - val_loss: 37.4910\n",
      "Epoch 30/1000\n",
      "0s - loss: 45.4906 - val_loss: 37.5385\n",
      "Epoch 31/1000\n",
      "0s - loss: 44.1979 - val_loss: 35.1584\n",
      "Epoch 32/1000\n",
      "0s - loss: 41.5529 - val_loss: 34.2374\n",
      "Epoch 33/1000\n",
      "0s - loss: 40.6663 - val_loss: 34.8862\n",
      "Epoch 34/1000\n",
      "0s - loss: 40.4157 - val_loss: 32.7954\n",
      "Epoch 35/1000\n",
      "0s - loss: 40.6478 - val_loss: 35.6296\n",
      "Epoch 36/1000\n",
      "0s - loss: 37.0193 - val_loss: 31.6542\n",
      "Epoch 37/1000\n",
      "0s - loss: 35.9128 - val_loss: 30.3670\n",
      "Epoch 38/1000\n",
      "0s - loss: 34.9990 - val_loss: 30.0664\n",
      "Epoch 39/1000\n",
      "0s - loss: 34.1329 - val_loss: 29.1883\n",
      "Epoch 40/1000\n",
      "0s - loss: 33.0751 - val_loss: 29.3028\n",
      "Epoch 41/1000\n",
      "0s - loss: 32.8258 - val_loss: 27.5401\n",
      "Epoch 42/1000\n",
      "0s - loss: 31.3282 - val_loss: 26.8245\n",
      "Epoch 43/1000\n",
      "0s - loss: 30.8449 - val_loss: 26.5097\n",
      "Epoch 44/1000\n",
      "0s - loss: 29.9476 - val_loss: 26.2393\n",
      "Epoch 45/1000\n",
      "0s - loss: 29.6841 - val_loss: 25.2146\n",
      "Epoch 46/1000\n",
      "0s - loss: 29.0431 - val_loss: 25.2830\n",
      "Epoch 47/1000\n",
      "0s - loss: 28.4375 - val_loss: 25.6998\n",
      "Epoch 48/1000\n",
      "0s - loss: 28.7064 - val_loss: 26.7102\n",
      "Epoch 49/1000\n",
      "0s - loss: 29.0178 - val_loss: 24.1067\n",
      "Epoch 50/1000\n",
      "0s - loss: 27.3361 - val_loss: 23.7308\n",
      "Epoch 51/1000\n",
      "0s - loss: 26.7959 - val_loss: 23.4185\n",
      "Epoch 52/1000\n",
      "0s - loss: 26.4071 - val_loss: 23.1151\n",
      "Epoch 53/1000\n",
      "0s - loss: 26.2440 - val_loss: 24.2807\n",
      "Epoch 54/1000\n",
      "0s - loss: 25.6648 - val_loss: 22.3755\n",
      "Epoch 55/1000\n",
      "0s - loss: 25.1381 - val_loss: 22.0063\n",
      "Epoch 56/1000\n",
      "0s - loss: 24.7699 - val_loss: 21.8958\n",
      "Epoch 57/1000\n",
      "0s - loss: 24.7163 - val_loss: 21.4892\n",
      "Epoch 58/1000\n",
      "0s - loss: 24.2166 - val_loss: 21.1210\n",
      "Epoch 59/1000\n",
      "0s - loss: 23.7183 - val_loss: 20.8771\n",
      "Epoch 60/1000\n",
      "0s - loss: 23.3688 - val_loss: 22.4518\n",
      "Epoch 61/1000\n",
      "0s - loss: 24.2960 - val_loss: 20.1415\n",
      "Epoch 62/1000\n",
      "0s - loss: 22.7843 - val_loss: 20.0782\n",
      "Epoch 63/1000\n",
      "0s - loss: 22.4448 - val_loss: 19.6148\n",
      "Epoch 64/1000\n",
      "0s - loss: 22.5731 - val_loss: 19.2631\n",
      "Epoch 65/1000\n",
      "0s - loss: 22.3160 - val_loss: 19.0837\n",
      "Epoch 66/1000\n",
      "0s - loss: 22.3610 - val_loss: 19.1579\n",
      "Epoch 67/1000\n",
      "0s - loss: 21.2752 - val_loss: 18.8465\n",
      "Epoch 68/1000\n",
      "0s - loss: 21.2627 - val_loss: 18.1025\n",
      "Epoch 69/1000\n",
      "0s - loss: 20.7018 - val_loss: 17.9497\n",
      "Epoch 70/1000\n",
      "0s - loss: 20.3331 - val_loss: 18.0582\n",
      "Epoch 71/1000\n",
      "0s - loss: 20.7485 - val_loss: 18.0760\n",
      "Epoch 72/1000\n",
      "0s - loss: 19.8726 - val_loss: 18.0067\n",
      "Epoch 73/1000\n",
      "0s - loss: 19.6031 - val_loss: 17.0164\n",
      "Epoch 74/1000\n",
      "0s - loss: 19.0825 - val_loss: 16.5432\n",
      "Epoch 75/1000\n",
      "0s - loss: 19.0614 - val_loss: 16.4859\n",
      "Epoch 76/1000\n",
      "0s - loss: 18.6531 - val_loss: 16.0804\n",
      "Epoch 77/1000\n",
      "0s - loss: 18.2185 - val_loss: 16.7702\n",
      "Epoch 78/1000\n",
      "0s - loss: 18.1993 - val_loss: 15.6688\n",
      "Epoch 79/1000\n",
      "0s - loss: 17.8713 - val_loss: 16.1116\n",
      "Epoch 80/1000\n",
      "0s - loss: 17.6029 - val_loss: 15.2819\n",
      "Epoch 81/1000\n",
      "0s - loss: 17.3415 - val_loss: 14.8937\n",
      "Epoch 82/1000\n",
      "0s - loss: 16.9664 - val_loss: 14.6213\n",
      "Epoch 83/1000\n",
      "0s - loss: 16.8470 - val_loss: 15.1330\n",
      "Epoch 84/1000\n",
      "0s - loss: 16.7432 - val_loss: 14.4343\n",
      "Epoch 85/1000\n",
      "0s - loss: 16.3047 - val_loss: 13.9447\n",
      "Epoch 86/1000\n",
      "0s - loss: 16.2791 - val_loss: 13.8690\n",
      "Epoch 87/1000\n",
      "0s - loss: 16.0939 - val_loss: 13.5522\n",
      "Epoch 88/1000\n",
      "0s - loss: 15.7043 - val_loss: 13.3369\n",
      "Epoch 89/1000\n",
      "0s - loss: 15.5220 - val_loss: 14.3028\n",
      "Epoch 90/1000\n",
      "0s - loss: 16.1095 - val_loss: 13.5941\n",
      "Epoch 91/1000\n",
      "0s - loss: 15.2041 - val_loss: 12.8103\n",
      "Epoch 92/1000\n",
      "0s - loss: 16.0562 - val_loss: 13.4900\n",
      "Epoch 93/1000\n",
      "0s - loss: 15.3422 - val_loss: 12.4145\n",
      "Epoch 94/1000\n",
      "0s - loss: 16.0101 - val_loss: 12.3813\n",
      "Epoch 95/1000\n",
      "0s - loss: 14.9585 - val_loss: 12.9629\n",
      "Epoch 96/1000\n",
      "0s - loss: 14.3549 - val_loss: 12.0329\n",
      "Epoch 97/1000\n",
      "0s - loss: 14.1106 - val_loss: 11.6814\n",
      "Epoch 98/1000\n",
      "0s - loss: 14.1620 - val_loss: 11.7210\n",
      "Epoch 99/1000\n",
      "0s - loss: 13.8978 - val_loss: 11.4636\n",
      "Epoch 100/1000\n",
      "0s - loss: 15.2452 - val_loss: 11.4632\n",
      "Epoch 101/1000\n",
      "0s - loss: 14.6605 - val_loss: 11.2422\n",
      "Epoch 102/1000\n",
      "0s - loss: 13.3962 - val_loss: 11.1073\n",
      "Epoch 103/1000\n",
      "0s - loss: 13.3570 - val_loss: 11.5303\n",
      "Epoch 104/1000\n",
      "0s - loss: 13.4745 - val_loss: 11.6723\n",
      "Epoch 105/1000\n",
      "0s - loss: 13.7500 - val_loss: 11.8663\n",
      "Epoch 106/1000\n",
      "0s - loss: 14.0648 - val_loss: 11.2297\n",
      "Epoch 107/1000\n",
      "0s - loss: 13.3102 - val_loss: 11.2606\n",
      "Epoch 108/1000\n",
      "0s - loss: 12.8517 - val_loss: 10.5795\n",
      "Epoch 109/1000\n",
      "0s - loss: 13.3428 - val_loss: 11.0975\n",
      "Epoch 110/1000\n",
      "0s - loss: 12.9045 - val_loss: 10.1282\n",
      "Epoch 111/1000\n",
      "0s - loss: 12.9028 - val_loss: 12.5454\n",
      "Epoch 112/1000\n",
      "0s - loss: 12.4937 - val_loss: 10.0316\n",
      "Epoch 113/1000\n",
      "0s - loss: 12.3144 - val_loss: 9.8566\n",
      "Epoch 114/1000\n",
      "0s - loss: 12.0633 - val_loss: 9.7592\n",
      "Epoch 115/1000\n",
      "0s - loss: 12.6366 - val_loss: 9.6706\n",
      "Epoch 116/1000\n",
      "0s - loss: 11.7865 - val_loss: 9.5543\n",
      "Epoch 117/1000\n",
      "0s - loss: 11.6379 - val_loss: 9.5803\n",
      "Epoch 118/1000\n",
      "0s - loss: 11.5879 - val_loss: 9.5570\n",
      "Epoch 119/1000\n",
      "0s - loss: 11.4610 - val_loss: 9.3698\n",
      "Epoch 120/1000\n",
      "0s - loss: 11.7794 - val_loss: 9.4475\n",
      "Epoch 121/1000\n",
      "0s - loss: 11.7551 - val_loss: 9.1281\n",
      "Epoch 122/1000\n",
      "0s - loss: 11.5885 - val_loss: 10.4637\n",
      "Epoch 123/1000\n",
      "0s - loss: 11.6149 - val_loss: 9.8029\n",
      "Epoch 124/1000\n",
      "0s - loss: 11.7957 - val_loss: 9.5561\n",
      "Epoch 125/1000\n",
      "0s - loss: 11.2886 - val_loss: 8.9604\n",
      "Epoch 126/1000\n",
      "0s - loss: 11.4636 - val_loss: 8.9400\n",
      "Epoch 127/1000\n",
      "0s - loss: 11.0559 - val_loss: 8.8907\n",
      "Epoch 128/1000\n",
      "0s - loss: 11.0570 - val_loss: 8.9012\n",
      "Epoch 129/1000\n",
      "0s - loss: 10.8876 - val_loss: 8.9476\n",
      "Epoch 130/1000\n",
      "0s - loss: 11.0729 - val_loss: 8.9078\n",
      "Epoch 131/1000\n",
      "0s - loss: 10.7703 - val_loss: 9.0967\n",
      "Epoch 132/1000\n",
      "0s - loss: 10.8610 - val_loss: 8.8116\n",
      "Epoch 133/1000\n",
      "0s - loss: 11.6487 - val_loss: 8.9328\n",
      "Epoch 134/1000\n",
      "0s - loss: 11.2148 - val_loss: 9.1293\n",
      "Epoch 135/1000\n",
      "0s - loss: 11.2153 - val_loss: 8.8363\n",
      "Epoch 136/1000\n",
      "0s - loss: 11.0686 - val_loss: 8.4907\n",
      "Epoch 137/1000\n",
      "0s - loss: 10.7374 - val_loss: 8.4988\n",
      "Epoch 138/1000\n",
      "0s - loss: 10.5892 - val_loss: 8.7477\n",
      "Epoch 139/1000\n",
      "0s - loss: 10.9700 - val_loss: 8.4721\n",
      "Epoch 140/1000\n",
      "0s - loss: 11.4909 - val_loss: 8.6080\n",
      "Epoch 141/1000\n",
      "0s - loss: 11.9004 - val_loss: 8.4772\n",
      "Epoch 142/1000\n",
      "0s - loss: 10.8814 - val_loss: 8.4717\n",
      "Epoch 143/1000\n",
      "0s - loss: 10.3017 - val_loss: 8.9541\n",
      "Epoch 144/1000\n",
      "0s - loss: 10.8235 - val_loss: 8.9946\n",
      "Epoch 145/1000\n",
      "0s - loss: 10.6930 - val_loss: 8.3718\n",
      "Epoch 146/1000\n",
      "0s - loss: 10.6211 - val_loss: 8.3512\n",
      "Epoch 147/1000\n",
      "0s - loss: 10.3855 - val_loss: 9.8734\n",
      "Epoch 148/1000\n",
      "0s - loss: 11.5088 - val_loss: 11.8793\n",
      "Epoch 149/1000\n",
      "0s - loss: 11.0046 - val_loss: 8.3620\n",
      "Epoch 150/1000\n",
      "0s - loss: 10.5731 - val_loss: 11.0854\n",
      "Epoch 151/1000\n",
      "0s - loss: 11.4336 - val_loss: 10.2352\n",
      "Epoch 152/1000\n",
      "0s - loss: 11.0783 - val_loss: 8.7475\n",
      "Epoch 00151: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c934a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "cars = df['name']\n",
    "df.drop('name',1,inplace=True)\n",
    "missing_median(df, 'horsepower')\n",
    "x,y = to_xy(df,\"mpg\")\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 2.95761775970459\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure MSE error.  \n",
    "score = metrics.mean_squared_error(pred,y_test)\n",
    "print(\"Final score (RMSE): {}\".format(np.sqrt(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/anaconda/envs/wustl/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/Users/jeff/anaconda/envs/wustl/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, 1))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s - loss: 0.7041 - acc: 0.3750\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s - loss: 0.6971 - acc: 0.5417\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s - loss: 0.6874 - acc: 0.7083\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s - loss: 0.6869 - acc: 0.7500\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s - loss: 0.6801 - acc: 0.6250\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s - loss: 0.6756 - acc: 0.7083\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s - loss: 0.6676 - acc: 0.7500\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s - loss: 0.6644 - acc: 0.7083\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s - loss: 0.6530 - acc: 0.7500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s - loss: 0.6436 - acc: 0.7500\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s - loss: 0.6385 - acc: 0.7500\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s - loss: 0.6180 - acc: 0.7500\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s - loss: 0.6076 - acc: 0.7500\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s - loss: 0.5971 - acc: 0.7500\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s - loss: 0.6303 - acc: 0.7500\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s - loss: 0.6164 - acc: 0.7500\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s - loss: 0.5883 - acc: 0.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s - loss: 0.6092 - acc: 0.7500\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s - loss: 0.5969 - acc: 0.7500\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s - loss: 0.5611 - acc: 0.7500\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s - loss: 0.5769 - acc: 0.7500\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s - loss: 0.5503 - acc: 0.7500\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s - loss: 0.5202 - acc: 0.7500\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s - loss: 0.5924 - acc: 0.7500\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s - loss: 0.5270 - acc: 0.7500\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s - loss: 0.5369 - acc: 0.7500\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s - loss: 0.5227 - acc: 0.7500\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s - loss: 0.5230 - acc: 0.7500\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s - loss: 0.5320 - acc: 0.7500\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s - loss: 0.5264 - acc: 0.7500\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s - loss: 0.5104 - acc: 0.7500\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s - loss: 0.5042 - acc: 0.7500\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s - loss: 0.5278 - acc: 0.7500\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s - loss: 0.4728 - acc: 0.7500\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s - loss: 0.4874 - acc: 0.7500\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s - loss: 0.4832 - acc: 0.7500\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s - loss: 0.5182 - acc: 0.7500\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s - loss: 0.4804 - acc: 0.7500\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s - loss: 0.4435 - acc: 0.7500\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s - loss: 0.4628 - acc: 0.7500\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s - loss: 0.4903 - acc: 0.7500\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s - loss: 0.4515 - acc: 0.7500\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s - loss: 0.5020 - acc: 0.7500\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s - loss: 0.4408 - acc: 0.7500\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s - loss: 0.4303 - acc: 0.7500\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s - loss: 0.4228 - acc: 0.7500\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s - loss: 0.4527 - acc: 0.7500\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s - loss: 0.4175 - acc: 0.7500\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s - loss: 0.4039 - acc: 0.7500\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s - loss: 0.4081 - acc: 0.7500\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s - loss: 0.4027 - acc: 0.7500\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s - loss: 0.4266 - acc: 0.7083\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s - loss: 0.3911 - acc: 0.8333\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s - loss: 0.4821 - acc: 0.7083\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s - loss: 0.4305 - acc: 0.7917\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s - loss: 0.4219 - acc: 0.8333\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s - loss: 0.3917 - acc: 0.7500\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s - loss: 0.3587 - acc: 0.9167\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s - loss: 0.4043 - acc: 0.7500\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s - loss: 0.3972 - acc: 0.8333\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s - loss: 0.4159 - acc: 0.7500\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s - loss: 0.3360 - acc: 0.8750\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s - loss: 0.4631 - acc: 0.6667\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s - loss: 0.3679 - acc: 0.8333\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s - loss: 0.4808 - acc: 0.7083\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s - loss: 0.4485 - acc: 0.7083\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s - loss: 0.4654 - acc: 0.7083\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s - loss: 0.3744 - acc: 0.7917\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s - loss: 0.4311 - acc: 0.7500\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s - loss: 0.3090 - acc: 0.9167\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s - loss: 0.3247 - acc: 0.8333\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s - loss: 0.4741 - acc: 0.7083\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s - loss: 0.3202 - acc: 0.8333\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s - loss: 0.4203 - acc: 0.8333\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s - loss: 0.2973 - acc: 0.9167\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s - loss: 0.3533 - acc: 0.8333\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s - loss: 0.4159 - acc: 0.8333\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s - loss: 0.3864 - acc: 0.7500\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s - loss: 0.3354 - acc: 0.8750\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s - loss: 0.4162 - acc: 0.7917\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s - loss: 0.3887 - acc: 0.8333\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s - loss: 0.3219 - acc: 0.9167\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s - loss: 0.2834 - acc: 0.9167\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s - loss: 0.3272 - acc: 0.9167\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s - loss: 0.2783 - acc: 0.9167\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s - loss: 0.3838 - acc: 0.8333\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s - loss: 0.3739 - acc: 0.8750\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s - loss: 0.3784 - acc: 0.8333\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s - loss: 0.4016 - acc: 0.8750\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s - loss: 0.2588 - acc: 0.9583\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s - loss: 0.3431 - acc: 0.8750\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s - loss: 0.2470 - acc: 0.9583\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s - loss: 0.4656 - acc: 0.7500\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s - loss: 0.2698 - acc: 0.9167\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s - loss: 0.2998 - acc: 0.9167\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s - loss: 0.3577 - acc: 0.8750\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s - loss: 0.2423 - acc: 0.9167\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s - loss: 0.3800 - acc: 0.8333\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s - loss: 0.4387 - acc: 0.7917\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s - loss: 0.4304 - acc: 0.7917\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s - loss: 0.2522 - acc: 0.9167\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s - loss: 0.2264 - acc: 0.9167\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s - loss: 0.2427 - acc: 0.9167\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s - loss: 0.3407 - acc: 0.8750\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s - loss: 0.2495 - acc: 0.9167\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s - loss: 0.2415 - acc: 0.9167\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s - loss: 0.3487 - acc: 0.8750\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s - loss: 0.2802 - acc: 0.8750\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s - loss: 0.2318 - acc: 0.9167\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s - loss: 0.2337 - acc: 0.9583\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s - loss: 0.2363 - acc: 0.9167\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s - loss: 0.6487 - acc: 0.7083\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s - loss: 0.3412 - acc: 0.8333\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s - loss: 0.2357 - acc: 0.9167\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s - loss: 0.1903 - acc: 0.9583\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s - loss: 0.2143 - acc: 0.9167\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s - loss: 0.3325 - acc: 0.8750\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s - loss: 0.3650 - acc: 0.8750\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s - loss: 0.2667 - acc: 0.9167\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s - loss: 0.1942 - acc: 0.9583\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s - loss: 0.2172 - acc: 0.9167\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s - loss: 0.4057 - acc: 0.8333\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s - loss: 0.1950 - acc: 0.9583\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s - loss: 0.1906 - acc: 0.9167\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s - loss: 0.3909 - acc: 0.8333\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s - loss: 0.5267 - acc: 0.7500\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s - loss: 0.6111 - acc: 0.6667\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s - loss: 0.1804 - acc: 0.9583\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s - loss: 0.2050 - acc: 0.9167\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s - loss: 0.3590 - acc: 0.8333\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s - loss: 0.2372 - acc: 0.8750\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s - loss: 0.1965 - acc: 0.9583\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s - loss: 0.3296 - acc: 0.8333\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s - loss: 0.3283 - acc: 0.8750\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s - loss: 0.1957 - acc: 0.9583\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s - loss: 0.2491 - acc: 0.9167\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s - loss: 0.1804 - acc: 0.9583\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s - loss: 0.1731 - acc: 0.9583\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s - loss: 0.2372 - acc: 0.9167\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s - loss: 0.1955 - acc: 0.9583\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s - loss: 0.3463 - acc: 0.8750\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s - loss: 0.1580 - acc: 0.9583\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s - loss: 0.1897 - acc: 0.9583\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s - loss: 0.4421 - acc: 0.7917\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s - loss: 0.1799 - acc: 0.9583\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s - loss: 0.2310 - acc: 0.9167\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s - loss: 0.1709 - acc: 0.9583\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s - loss: 0.4492 - acc: 0.7917\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s - loss: 0.1447 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s - loss: 0.3036 - acc: 0.8750\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s - loss: 0.3216 - acc: 0.8750\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s - loss: 0.2916 - acc: 0.8750\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s - loss: 0.2669 - acc: 0.9167\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s - loss: 0.3831 - acc: 0.8333\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s - loss: 0.2742 - acc: 0.9167\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s - loss: 0.1575 - acc: 0.9583\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s - loss: 0.3612 - acc: 0.8333\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s - loss: 0.1545 - acc: 0.9583\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s - loss: 0.1513 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s - loss: 0.1380 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s - loss: 0.2955 - acc: 0.8750\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s - loss: 0.2367 - acc: 0.9167\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s - loss: 0.3018 - acc: 0.8333\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s - loss: 0.3612 - acc: 0.8333\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s - loss: 0.1474 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s - loss: 0.1399 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s - loss: 0.2590 - acc: 0.9167\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s - loss: 0.1262 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s - loss: 0.2189 - acc: 0.9167\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s - loss: 0.1313 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s - loss: 0.1544 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s - loss: 0.1247 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s - loss: 0.1517 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s - loss: 0.1802 - acc: 0.9167\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s - loss: 0.1519 - acc: 0.9583\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s - loss: 0.1246 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s - loss: 0.3876 - acc: 0.8333\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s - loss: 0.1888 - acc: 0.9167\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s - loss: 0.2474 - acc: 0.9167\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s - loss: 0.1336 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s - loss: 0.0953 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s - loss: 0.2950 - acc: 0.9167\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s - loss: 0.2576 - acc: 0.9167\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s - loss: 0.2459 - acc: 0.9167\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s - loss: 0.3849 - acc: 0.8333\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s - loss: 0.2749 - acc: 0.9167\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s - loss: 0.2753 - acc: 0.8750\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s - loss: 0.1196 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s - loss: 0.2251 - acc: 0.9167\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s - loss: 0.3815 - acc: 0.8333\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s - loss: 0.2155 - acc: 0.9167\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s - loss: 0.3169 - acc: 0.8333\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s - loss: 0.1259 - acc: 1.0000\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s - loss: 0.1183 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s - loss: 0.2428 - acc: 0.9167\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s - loss: 0.1045 - acc: 0.9583\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s - loss: 0.0911 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s - loss: 0.1424 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s - loss: 0.1818 - acc: 0.9167\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s - loss: 0.1514 - acc: 0.9583\n",
      "Predicted classes: {} [1 2 3 2 3 1]\n",
      "Expected classes: {} [1 2 3 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "max_features = 4 # 0,1,2,3 (total of 4)\n",
    "x = [\n",
    "    [[0],[1],[1],[0],[0],[0]],\n",
    "    [[0],[0],[0],[2],[2],[0]],\n",
    "    [[0],[0],[0],[0],[3],[3]],\n",
    "    [[0],[2],[2],[0],[0],[0]],\n",
    "    [[0],[0],[3],[3],[0],[0]],\n",
    "    [[0],[0],[0],[0],[1],[1]]\n",
    "]\n",
    "x = np.array(x,dtype=np.float32)\n",
    "y = np.array([1,2,3,2,3,1],dtype=np.int32)\n",
    "\n",
    "# Convert y2 to dummy variables\n",
    "y2 = np.zeros((y.shape[0], max_features),dtype=np.float32)\n",
    "y2[np.arange(y.shape[0]), y] = 1.0\n",
    "print(y2)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_dim=1))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x,y2,epochs=200)\n",
    "pred = model.predict(x)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "print(\"Predicted classes: {}\",predict_classes)\n",
    "print(\"Expected classes: {}\",predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def runit(model, inp):\n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    pred = model.predict(inp)\n",
    "    return np.argmax(pred[0])\n",
    "\n",
    "print( runit( model, [[[0],[0],[0],[0],[3],[3]]] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
